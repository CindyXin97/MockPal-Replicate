import { neon } from '@neondatabase/serverless';
import dotenv from 'dotenv';

dotenv.config({ path: '.env.local' });
const sql = neon(process.env.DATABASE_URL!);

// Python/SQLç¼–ç¨‹é¢˜ç›®
const pythonSqlQuestions = [
  // Python æ•°æ®å¤„ç†é¢˜ç›® (5é“)
  {
    company: 'Meta',
    position: 'æ•°æ®ç§‘å­¦å®¶',
    questionType: 'technical',
    difficulty: 'medium',
    question: 'ç»™å®šä¸€ä¸ªåŒ…å«ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„DataFrameï¼Œè®¡ç®—æ¯ä¸ªç”¨æˆ·çš„7å¤©æ»šåŠ¨æ´»è·ƒåº¦ã€‚è¦æ±‚å¤„ç†ç¼ºå¤±å€¼å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚',
    recommendedAnswer: `# è®¡ç®—ç”¨æˆ·7å¤©æ»šåŠ¨æ´»è·ƒåº¦
import pandas as pd
import numpy as np

def calculate_rolling_activity(df):
    """
    è®¡ç®—ç”¨æˆ·7å¤©æ»šåŠ¨æ´»è·ƒåº¦
    df: åŒ…å« user_id, date, activity_score åˆ—
    """
    # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
    df['date'] = pd.to_datetime(df['date'])
    
    # æŒ‰ç”¨æˆ·å’Œæ—¥æœŸæ’åº
    df = df.sort_values(['user_id', 'date'])
    
    # å¤„ç†ç¼ºå¤±å€¼ - ç”¨0å¡«å……æ´»è·ƒåº¦åˆ†æ•°
    df['activity_score'] = df['activity_score'].fillna(0)
    
    # è®¡ç®—7å¤©æ»šåŠ¨å¹³å‡
    df['rolling_7d_activity'] = (df.groupby('user_id')['activity_score']
                                  .rolling(window=7, min_periods=1)
                                  .mean()
                                  .reset_index(0, drop=True))
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨å‘å‰å¡«å……å¤„ç†æ—¥æœŸé—´éš”
    df = df.set_index(['user_id', 'date']).sort_index()
    
    return df

# ä½¿ç”¨ç¤ºä¾‹
# df_result = calculate_rolling_activity(user_behavior_df)`,
    tags: 'pandas,rolling,æ•°æ®å¤„ç†',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'Google',
    position: 'æ•°æ®åˆ†æå¸ˆ',
    questionType: 'technical',
    difficulty: 'medium',
    question: 'ç¼–å†™Pythonä»£ç æ¥æ£€æµ‹æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œä½¿ç”¨ç»Ÿè®¡æ–¹æ³•å’Œæœºå™¨å­¦ä¹ æ–¹æ³•å„å®ç°ä¸€ç§ã€‚',
    recommendedAnswer: `# æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from scipy import stats

def detect_anomalies_statistical(ts_data, threshold=2.5):
    """ä½¿ç”¨Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
    z_scores = np.abs(stats.zscore(ts_data))
    anomalies = z_scores > threshold
    return anomalies, z_scores

def detect_anomalies_ml(ts_data, contamination=0.1):
    """ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸å€¼"""
    df = pd.DataFrame({'value': ts_data})
    
    # åˆ›å»ºç‰¹å¾ï¼šæ»‘åŠ¨ç»Ÿè®¡
    df['rolling_mean'] = df['value'].rolling(window=5).mean()
    df['rolling_std'] = df['value'].rolling(window=5).std()
    df['lag_1'] = df['value'].shift(1)
    
    features = df[['value', 'rolling_mean', 'rolling_std', 'lag_1']].dropna()
    
    # è®­ç»ƒæ¨¡å‹
    iso_forest = IsolationForest(contamination=contamination, random_state=42)
    anomalies = iso_forest.fit_predict(features)
    anomalies = anomalies == -1
    
    return anomalies, iso_forest.decision_function(features)

# ä½¿ç”¨ç¤ºä¾‹
# ts = [1, 2, 3, 2, 1, 50, 2, 3, 1, 2]  # 50æ˜¯å¼‚å¸¸å€¼
# stat_anomalies, z_scores = detect_anomalies_statistical(ts)
# ml_anomalies, scores = detect_anomalies_ml(ts)`,
    tags: 'anomaly_detection,æ—¶é—´åºåˆ—,æœºå™¨å­¦ä¹ ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'Amazon',
    position: 'æ•°æ®ç§‘å­¦å®¶',
    questionType: 'technical',
    difficulty: 'hard',
    question: 'å®ç°ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—ç”¨æˆ·è´­ä¹°è¡Œä¸ºçš„RFMåˆ†æï¼Œå¹¶æ ¹æ®RFMåˆ†æ•°è¿›è¡Œå®¢æˆ·åˆ†ç¾¤ã€‚',
    recommendedAnswer: `# RFMå®¢æˆ·åˆ†ç¾¤åˆ†æ
import pandas as pd
import numpy as np
from datetime import datetime

def rfm_analysis(df, customer_id='customer_id', order_date='order_date', revenue='revenue'):
    """RFMåˆ†æï¼šRecency, Frequency, Monetary"""
    # ç¡®ä¿æ—¥æœŸæ ¼å¼
    df[order_date] = pd.to_datetime(df[order_date])
    
    # è®¡ç®—åˆ†æåŸºå‡†æ—¥æœŸ
    snapshot_date = df[order_date].max() + pd.Timedelta(days=1)
    
    # è®¡ç®—RFMæŒ‡æ ‡
    rfm = df.groupby(customer_id).agg({
        order_date: lambda x: (snapshot_date - x.max()).days,  # Recency
        customer_id: 'count',  # Frequency  
        revenue: 'sum'  # Monetary
    }).rename(columns={
        order_date: 'Recency',
        customer_id: 'Frequency', 
        revenue: 'Monetary'
    })
    
    # è®¡ç®—RFMåˆ†æ•° (1-5åˆ†)
    rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5,4,3,2,1])
    rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
    rfm['M_Score'] = pd.qcut(rfm['Monetary'], 5, labels=[1,2,3,4,5])
    
    # ç»„åˆRFMåˆ†æ•°
    rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)
    
    # å®¢æˆ·åˆ†ç¾¤
    def segment_customers(row):
        if row['RFM_Score'] in ['555', '554', '544', '545']:
            return 'Champions'
        elif row['RFM_Score'] in ['543', '444', '435', '355']:
            return 'Loyal Customers'
        elif row['RFM_Score'] in ['512', '511', '422', '421']:
            return 'Potential Loyalists'
        else:
            return 'Others'
    
    rfm['Segment'] = rfm.apply(segment_customers, axis=1)
    return rfm

# ä½¿ç”¨ç¤ºä¾‹
# rfm_result = rfm_analysis(transaction_df)`,
    tags: 'RFM,å®¢æˆ·åˆ†æ,pandas',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'Uber',
    position: 'æ•°æ®ç§‘å­¦å®¶',
    questionType: 'technical',
    difficulty: 'medium',
    question: 'ç¼–å†™Pythonä»£ç å®ç°A/Bæµ‹è¯•çš„ç»Ÿè®¡åˆ†æï¼ŒåŒ…æ‹¬æ ·æœ¬é‡è®¡ç®—ã€å‡è®¾æ£€éªŒå’Œç½®ä¿¡åŒºé—´ã€‚',
    recommendedAnswer: `# A/Bæµ‹è¯•ç»Ÿè®¡åˆ†æ
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import norm
import math

class ABTestAnalyzer:
    def __init__(self, alpha=0.05, power=0.8):
        self.alpha = alpha
        self.power = power
        
    def sample_size_calculation(self, baseline_rate, effect_size):
        """è®¡ç®—A/Bæµ‹è¯•æ‰€éœ€æ ·æœ¬é‡"""
        new_rate = baseline_rate * (1 + effect_size)
        
        z_alpha = norm.ppf(1 - self.alpha/2)
        z_beta = norm.ppf(self.power)
        
        pooled_rate = (baseline_rate + new_rate) / 2
        
        numerator = (z_alpha * math.sqrt(2 * pooled_rate * (1 - pooled_rate)) + 
                    z_beta * math.sqrt(baseline_rate * (1 - baseline_rate) + 
                                     new_rate * (1 - new_rate))) ** 2
        
        denominator = (new_rate - baseline_rate) ** 2
        n = numerator / denominator
        
        return math.ceil(n)
    
    def proportion_test(self, control_conversions, control_total, 
                       treatment_conversions, treatment_total):
        """æ¯”ä¾‹çš„å‡è®¾æ£€éªŒ"""
        p1 = control_conversions / control_total
        p2 = treatment_conversions / treatment_total
        
        # åˆå¹¶æ¯”ä¾‹
        p_pool = (control_conversions + treatment_conversions) / (control_total + treatment_total)
        
        # æ ‡å‡†è¯¯å·®
        se = math.sqrt(p_pool * (1 - p_pool) * (1/control_total + 1/treatment_total))
        
        # Zç»Ÿè®¡é‡
        z_stat = (p2 - p1) / se
        p_value = 2 * (1 - norm.cdf(abs(z_stat)))
        
        # ç½®ä¿¡åŒºé—´
        ci_se = math.sqrt(p1 * (1 - p1) / control_total + p2 * (1 - p2) / treatment_total)
        margin_error = norm.ppf(1 - self.alpha/2) * ci_se
        ci_lower = (p2 - p1) - margin_error
        ci_upper = (p2 - p1) + margin_error
        
        return {
            'control_rate': p1,
            'treatment_rate': p2,
            'z_statistic': z_stat,
            'p_value': p_value,
            'significant': p_value < self.alpha,
            'confidence_interval': (ci_lower, ci_upper)
        }

# ä½¿ç”¨ç¤ºä¾‹
ab_analyzer = ABTestAnalyzer()
sample_size = ab_analyzer.sample_size_calculation(baseline_rate=0.10, effect_size=0.20)
result = ab_analyzer.proportion_test(100, 1000, 130, 1000)`,
    tags: 'ABæµ‹è¯•,å‡è®¾æ£€éªŒ,ç»Ÿè®¡åˆ†æ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'Netflix',
    position: 'æ•°æ®ç§‘å­¦å®¶',
    questionType: 'technical',
    difficulty: 'hard',
    question: 'å®ç°ä¸€ä¸ªæ¨èç³»ç»Ÿçš„ååŒè¿‡æ»¤ç®—æ³•ï¼ŒåŒ…æ‹¬ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—å’Œæ¨èç”Ÿæˆã€‚è¦æ±‚å¤„ç†ç¨€ç–çŸ©é˜µä¼˜åŒ–æ€§èƒ½ã€‚',
    recommendedAnswer: `# ååŒè¿‡æ»¤æ¨èç³»ç»Ÿ
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity

class CollaborativeFiltering:
    def __init__(self, n_recommendations=10):
        self.n_recommendations = n_recommendations
        self.user_item_matrix = None
        self.user_similarity = None
        
    def fit(self, ratings_df):
        """è®­ç»ƒååŒè¿‡æ»¤æ¨¡å‹"""
        # åˆ›å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
        self.user_item_matrix = ratings_df.pivot(
            index='user_id', 
            columns='item_id', 
            values='rating'
        ).fillna(0)
        
        # è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µä¼˜åŒ–å†…å­˜
        sparse_matrix = csr_matrix(self.user_item_matrix.values)
        
        # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ
        self.user_similarity = cosine_similarity(sparse_matrix)
        self.user_similarity = pd.DataFrame(
            self.user_similarity,
            index=self.user_item_matrix.index,
            columns=self.user_item_matrix.index
        )
        
    def get_user_recommendations(self, user_id, n_similar_users=50):
        """ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆæ¨è"""
        if user_id not in self.user_item_matrix.index:
            return []
            
        # è·å–ç›¸ä¼¼ç”¨æˆ·
        user_similarities = self.user_similarity[user_id].sort_values(ascending=False)
        similar_users = user_similarities.iloc[1:n_similar_users+1].index
        
        # è·å–å·²è¯„åˆ†ç‰©å“
        user_ratings = self.user_item_matrix.loc[user_id]
        rated_items = user_ratings[user_ratings > 0].index
        
        # è®¡ç®—æ¨èåˆ†æ•°
        recommendations = {}
        
        for item in self.user_item_matrix.columns:
            if item in rated_items:
                continue
                
            weighted_sum = 0
            similarity_sum = 0
            
            for similar_user in similar_users:
                if self.user_item_matrix.loc[similar_user, item] > 0:
                    similarity = user_similarities[similar_user]
                    rating = self.user_item_matrix.loc[similar_user, item]
                    
                    weighted_sum += similarity * rating
                    similarity_sum += abs(similarity)
            
            if similarity_sum > 0:
                recommendations[item] = weighted_sum / similarity_sum
        
        # è¿”å›top-Næ¨è
        sorted_recommendations = sorted(
            recommendations.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return sorted_recommendations[:self.n_recommendations]

# ä½¿ç”¨ç¤ºä¾‹
# cf = CollaborativeFiltering()
# cf.fit(ratings_df)
# recommendations = cf.get_user_recommendations(user_id=123)`,
    tags: 'ååŒè¿‡æ»¤,æ¨èç³»ç»Ÿ,ç¨€ç–çŸ©é˜µ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },

  // SQL æŸ¥è¯¢é¢˜ç›® (5é“)
  {
    company: 'Meta',
    position: 'æ•°æ®åˆ†æå¸ˆ',
    questionType: 'technical',
    difficulty: 'medium',
    question: 'ç¼–å†™SQLæŸ¥è¯¢æ¥è®¡ç®—ç”¨æˆ·ç•™å­˜ç‡ï¼šè®¡ç®—æ¯ä¸ªæœˆæ–°æ³¨å†Œç”¨æˆ·åœ¨ç¬¬1ã€7ã€30å¤©çš„ç•™å­˜ç‡ã€‚',
    recommendedAnswer: `-- ç”¨æˆ·ç•™å­˜ç‡åˆ†æ
WITH user_first_login AS (
    -- è·å–æ¯ä¸ªç”¨æˆ·çš„é¦–æ¬¡ç™»å½•æ—¥æœŸ
    SELECT 
        user_id,
        DATE(MIN(login_date)) AS first_login_date,
        DATE_TRUNC('month', MIN(login_date)) AS cohort_month
    FROM user_activity
    GROUP BY user_id
),

user_activity_with_cohort AS (
    -- å°†ç”¨æˆ·æ´»åŠ¨ä¸é¦–æ¬¡ç™»å½•æ—¥æœŸå…³è”
    SELECT 
        ua.user_id,
        ua.login_date,
        ufl.first_login_date,
        ufl.cohort_month,
        DATE_DIFF(ua.login_date, ufl.first_login_date, DAY) AS days_since_first_login
    FROM user_activity ua
    JOIN user_first_login ufl ON ua.user_id = ufl.user_id
),

retention_data AS (
    -- è®¡ç®—å„ä¸ªæ—¶é—´ç‚¹çš„ç•™å­˜ç”¨æˆ·
    SELECT 
        cohort_month,
        COUNT(DISTINCT user_id) AS total_users,
        COUNT(DISTINCT CASE WHEN days_since_first_login = 1 THEN user_id END) AS day_1_users,
        COUNT(DISTINCT CASE WHEN days_since_first_login = 7 THEN user_id END) AS day_7_users,
        COUNT(DISTINCT CASE WHEN days_since_first_login = 30 THEN user_id END) AS day_30_users
    FROM user_activity_with_cohort
    GROUP BY cohort_month
)

-- è®¡ç®—ç•™å­˜ç‡
SELECT 
    cohort_month,
    total_users,
    ROUND(day_1_users * 100.0 / total_users, 2) AS day_1_retention_rate,
    ROUND(day_7_users * 100.0 / total_users, 2) AS day_7_retention_rate,
    ROUND(day_30_users * 100.0 / total_users, 2) AS day_30_retention_rate
FROM retention_data
WHERE total_users >= 100
ORDER BY cohort_month;`,
    tags: 'ç•™å­˜åˆ†æ,cohortåˆ†æ,çª—å£å‡½æ•°',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'Google',
    position: 'æ•°æ®åˆ†æå¸ˆ',
    questionType: 'technical',
    difficulty: 'hard',
    question: 'ç¼–å†™SQLæŸ¥è¯¢æ¥è¯†åˆ«å¼‚å¸¸çš„ç”¨æˆ·è¡Œä¸ºï¼šæ‰¾å‡ºæœç´¢é‡çªç„¶å¢åŠ è¶…è¿‡å¹³å‡æ°´å¹³3å€çš„ç”¨æˆ·ï¼Œå¹¶åˆ†æå…¶æœç´¢æ¨¡å¼ã€‚',
    recommendedAnswer: `-- å¼‚å¸¸ç”¨æˆ·æœç´¢è¡Œä¸ºåˆ†æ
WITH user_daily_searches AS (
    -- è®¡ç®—æ¯ä¸ªç”¨æˆ·æ¯å¤©çš„æœç´¢æ¬¡æ•°
    SELECT 
        user_id,
        DATE(search_timestamp) AS search_date,
        COUNT(*) AS daily_search_count,
        COUNT(DISTINCT search_query) AS unique_queries,
        AVG(LENGTH(search_query)) AS avg_query_length
    FROM search_logs
    WHERE search_timestamp >= CURRENT_DATE - INTERVAL '30 days'
    GROUP BY user_id, DATE(search_timestamp)
),

user_search_stats AS (
    -- è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æœç´¢ç»Ÿè®¡ä¿¡æ¯
    SELECT 
        user_id,
        AVG(daily_search_count) AS avg_daily_searches,
        STDDEV(daily_search_count) AS stddev_daily_searches,
        MAX(daily_search_count) AS max_daily_searches
    FROM user_daily_searches
    GROUP BY user_id
    HAVING COUNT(*) >= 7  -- è‡³å°‘æ´»è·ƒ7å¤©
),

anomalous_users AS (
    -- è¯†åˆ«å¼‚å¸¸ç”¨æˆ·
    SELECT 
        uss.user_id,
        uss.avg_daily_searches,
        uds.search_date AS anomaly_date,
        uds.daily_search_count AS anomaly_search_count,
        ROUND(uds.daily_search_count / uss.avg_daily_searches, 2) AS search_multiplier
    FROM user_search_stats uss
    JOIN user_daily_searches uds ON uss.user_id = uds.user_id
    WHERE uds.daily_search_count > uss.avg_daily_searches * 3
        AND uss.avg_daily_searches >= 5
)

-- åˆ†æå¼‚å¸¸ç”¨æˆ·çš„æœç´¢æ¨¡å¼
SELECT 
    au.user_id,
    au.anomaly_date,
    au.search_multiplier,
    COUNT(*) AS total_searches,
    COUNT(DISTINCT sl.search_query) AS unique_queries,
    AVG(LENGTH(sl.search_query)) AS avg_query_length,
    -- åˆ¤æ–­å¼‚å¸¸ç±»å‹
    CASE 
        WHEN COUNT(DISTINCT EXTRACT(HOUR FROM sl.search_timestamp)) <= 2 THEN 'Concentrated_Time'
        WHEN COUNT(DISTINCT sl.search_query) / COUNT(*) < 0.3 THEN 'Repetitive_Queries'
        WHEN AVG(LENGTH(sl.search_query)) < 3 THEN 'Short_Queries'
        ELSE 'Other'
    END AS anomaly_pattern
FROM anomalous_users au
JOIN search_logs sl ON au.user_id = sl.user_id 
    AND DATE(sl.search_timestamp) = au.anomaly_date
GROUP BY au.user_id, au.anomaly_date, au.search_multiplier
ORDER BY search_multiplier DESC;`,
    tags: 'å¼‚å¸¸æ£€æµ‹,ç”¨æˆ·è¡Œä¸ºåˆ†æ,ç»Ÿè®¡åˆ†æ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'Amazon',
    position: 'æ•°æ®åˆ†æå¸ˆ',
    questionType: 'technical',
    difficulty: 'medium',
    question: 'ç¼–å†™SQLæŸ¥è¯¢æ¥åˆ†æå•†å“æ¨èæ•ˆæœï¼šè®¡ç®—æ¨èå•†å“çš„ç‚¹å‡»ç‡ã€è½¬åŒ–ç‡ï¼Œå¹¶æŒ‰å•†å“ç±»åˆ«å’Œæ¨èä½ç½®è¿›è¡Œåˆ†æã€‚',
    recommendedAnswer: `-- å•†å“æ¨èæ•ˆæœåˆ†æ
WITH recommendation_events AS (
    -- è·å–æ¨èå±•ç¤ºã€ç‚¹å‡»ã€è´­ä¹°äº‹ä»¶
    SELECT 
        r.recommendation_id,
        r.user_id,
        r.product_id,
        r.recommendation_position,
        r.shown_timestamp,
        p.category,
        p.price,
        -- ç‚¹å‡»äº‹ä»¶
        CASE WHEN c.clicked_timestamp IS NOT NULL THEN 1 ELSE 0 END AS clicked,
        -- è´­ä¹°äº‹ä»¶
        CASE WHEN o.order_timestamp IS NOT NULL THEN 1 ELSE 0 END AS converted,
        o.revenue
    FROM recommendations r
    LEFT JOIN products p ON r.product_id = p.product_id
    LEFT JOIN clicks c ON r.recommendation_id = c.recommendation_id
        AND c.clicked_timestamp BETWEEN r.shown_timestamp AND r.shown_timestamp + INTERVAL '1 day'
    LEFT JOIN orders o ON r.user_id = o.user_id AND r.product_id = o.product_id
        AND o.order_timestamp BETWEEN r.shown_timestamp AND r.shown_timestamp + INTERVAL '7 days'
    WHERE r.shown_timestamp >= CURRENT_DATE - INTERVAL '30 days'
),

recommendation_metrics AS (
    -- è®¡ç®—åŸºç¡€æŒ‡æ ‡
    SELECT 
        category,
        recommendation_position,
        COUNT(*) AS total_recommendations,
        SUM(clicked) AS total_clicks,
        SUM(converted) AS total_conversions,
        SUM(revenue) AS total_revenue,
        -- è®¡ç®—ç‡
        ROUND(SUM(clicked) * 100.0 / COUNT(*), 2) AS click_rate,
        ROUND(SUM(converted) * 100.0 / COUNT(*), 2) AS conversion_rate,
        ROUND(SUM(converted) * 100.0 / NULLIF(SUM(clicked), 0), 2) AS click_to_conversion_rate,
        -- æ”¶å…¥æŒ‡æ ‡
        ROUND(SUM(revenue) / COUNT(*), 2) AS revenue_per_recommendation
    FROM recommendation_events
    GROUP BY category, recommendation_position
)

-- ä¸»æŸ¥è¯¢ï¼šæ¨èæ•ˆæœåˆ†æ
SELECT 
    category,
    recommendation_position,
    total_recommendations,
    click_rate,
    conversion_rate,
    click_to_conversion_rate,
    revenue_per_recommendation,
    -- æ•ˆæœè¯„çº§
    CASE 
        WHEN click_rate > 5 AND conversion_rate > 1 THEN 'Excellent'
        WHEN click_rate > 2 AND conversion_rate > 0.5 THEN 'Good'
        WHEN click_rate > 1 AND conversion_rate > 0.1 THEN 'Average'
        ELSE 'Poor'
    END AS performance_rating
FROM recommendation_metrics
WHERE total_recommendations >= 100
ORDER BY conversion_rate DESC, click_rate DESC;`,
    tags: 'æ¨èç³»ç»Ÿåˆ†æ,è½¬åŒ–ç‡åˆ†æ,å•†å“åˆ†æ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'TikTok',
    position: 'æ•°æ®åˆ†æå¸ˆ',
    questionType: 'technical',
    difficulty: 'hard',
    question: 'ç¼–å†™SQLæŸ¥è¯¢æ¥åˆ†æè§†é¢‘ç—…æ¯’å¼ä¼ æ’­ï¼šè¯†åˆ«ç—…æ¯’è§†é¢‘çš„ç‰¹å¾ï¼Œåˆ†æä¼ æ’­è·¯å¾„å’Œå½±å“å› å­ã€‚',
    recommendedAnswer: `-- è§†é¢‘ç—…æ¯’å¼ä¼ æ’­åˆ†æ
WITH video_metrics AS (
    -- è®¡ç®—è§†é¢‘çš„åŸºç¡€æŒ‡æ ‡
    SELECT 
        video_id,
        creator_id,
        upload_timestamp,
        COUNT(DISTINCT user_id) AS unique_viewers,
        COUNT(*) AS total_views,
        SUM(liked) AS total_likes,
        SUM(shared) AS total_shares,
        SUM(commented) AS total_comments,
        -- è®¡ç®—å‚ä¸ç‡
        ROUND(SUM(liked + shared + commented) * 100.0 / COUNT(*), 2) AS engagement_rate
    FROM video_views
    WHERE view_timestamp >= upload_timestamp
    GROUP BY video_id, creator_id, upload_timestamp
),

viral_videos AS (
    -- è¯†åˆ«ç—…æ¯’è§†é¢‘
    SELECT 
        vm.*,
        -- ç—…æ¯’æŒ‡æ•°è®¡ç®—
        ROUND(
            (LOG(unique_viewers) * 0.3 + 
             LOG(total_shares + 1) * 0.4 + 
             engagement_rate * 0.3), 2
        ) AS viral_score,
        -- åˆ†äº«ä¼ æ’­æ¯”
        ROUND(total_shares * 1.0 / unique_viewers, 4) AS share_rate,
        -- ç—…æ¯’ç­‰çº§
        CASE 
            WHEN unique_viewers >= 1000000 AND total_shares >= 50000 THEN 'Super_Viral'
            WHEN unique_viewers >= 500000 AND total_shares >= 20000 THEN 'Highly_Viral' 
            WHEN unique_viewers >= 100000 AND total_shares >= 5000 THEN 'Viral'
            ELSE 'Normal'
        END AS viral_level
    FROM video_metrics vm
    WHERE unique_viewers >= 10000
),

time_series_growth AS (
    -- åˆ†æè§†é¢‘å¢é•¿æ›²çº¿
    SELECT 
        vv.video_id,
        DATE_TRUNC('hour', view_timestamp) AS hour_bucket,
        COUNT(*) AS hourly_views,
        -- ç´¯è®¡æŒ‡æ ‡
        SUM(COUNT(*)) OVER (
            PARTITION BY vv.video_id 
            ORDER BY DATE_TRUNC('hour', view_timestamp)
        ) AS cumulative_views
    FROM viral_videos vv
    JOIN video_views vw ON vv.video_id = vw.video_id
    WHERE view_timestamp BETWEEN upload_timestamp AND upload_timestamp + INTERVAL '7 days'
    GROUP BY vv.video_id, DATE_TRUNC('hour', view_timestamp)
)

-- ä¸»æŸ¥è¯¢ï¼šç—…æ¯’è§†é¢‘ç»¼åˆåˆ†æ
SELECT 
    vv.video_id,
    vv.viral_level,
    vv.viral_score,
    vv.unique_viewers,
    vv.total_shares,
    vv.share_rate,
    vv.engagement_rate,
    -- å¢é•¿ç‰¹å¾
    MAX(tsg.hourly_views) AS peak_hourly_views,
    MAX(tsg.cumulative_views) AS final_cumulative_views
FROM viral_videos vv
LEFT JOIN time_series_growth tsg ON vv.video_id = tsg.video_id
WHERE vv.viral_level IN ('Viral', 'Highly_Viral', 'Super_Viral')
GROUP BY vv.video_id, vv.viral_level, vv.viral_score, vv.unique_viewers, 
         vv.total_shares, vv.share_rate, vv.engagement_rate
ORDER BY vv.viral_score DESC;`,
    tags: 'ç—…æ¯’ä¼ æ’­åˆ†æ,ç¤¾äº¤ç½‘ç»œåˆ†æ,å†…å®¹åˆ†æ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  },
  {
    company: 'LinkedIn',
    position: 'æ•°æ®åˆ†æå¸ˆ',
    questionType: 'technical',
    difficulty: 'medium',
    question: 'ç¼–å†™SQLæŸ¥è¯¢æ¥åˆ†æèŒä½æ¨èçš„ç²¾å‡†åº¦ï¼šè®¡ç®—æ¨èå‡†ç¡®ç‡ï¼Œåˆ†æç”¨æˆ·æŠ€èƒ½åŒ¹é…åº¦å¯¹æ¨èæ•ˆæœçš„å½±å“ã€‚',
    recommendedAnswer: `-- èŒä½æ¨èç²¾å‡†åº¦åˆ†æ
WITH user_skills AS (
    -- è·å–ç”¨æˆ·æŠ€èƒ½ä¿¡æ¯
    SELECT 
        user_id,
        skill_name,
        proficiency_level,
        years_experience
    FROM user_skills
    WHERE is_active = true
),

job_requirements AS (
    -- è·å–èŒä½æŠ€èƒ½è¦æ±‚
    SELECT 
        job_id,
        required_skill,
        importance_level,  -- 1-5, 5æœ€é‡è¦
        min_years_required
    FROM job_skill_requirements
),

recommendation_events AS (
    -- æ¨èäº‹ä»¶åŠç”¨æˆ·åé¦ˆ
    SELECT 
        r.recommendation_id,
        r.user_id,
        r.job_id,
        r.recommendation_score,
        -- ç”¨æˆ·è¡Œä¸º
        CASE WHEN c.clicked_timestamp IS NOT NULL THEN 1 ELSE 0 END AS clicked,
        CASE WHEN a.applied_timestamp IS NOT NULL THEN 1 ELSE 0 END AS applied,
        -- ç”¨æˆ·åé¦ˆ
        f.relevance_rating  -- 1-5åˆ†
    FROM recommendations r
    LEFT JOIN clicks c ON r.recommendation_id = c.recommendation_id
    LEFT JOIN applications a ON r.user_id = a.user_id AND r.job_id = a.job_id
    LEFT JOIN user_feedback f ON r.recommendation_id = f.recommendation_id
    WHERE r.recommended_timestamp >= CURRENT_DATE - INTERVAL '30 days'
),

skill_matching AS (
    -- è®¡ç®—ç”¨æˆ·ä¸èŒä½çš„æŠ€èƒ½åŒ¹é…åº¦
    SELECT 
        re.recommendation_id,
        re.user_id,
        re.job_id,
        COUNT(jr.required_skill) AS total_required_skills,
        COUNT(us.skill_name) AS matched_skills,
        -- åŒ¹é…åº¦è®¡ç®—
        ROUND(COUNT(us.skill_name) * 100.0 / COUNT(jr.required_skill), 2) AS skill_match_percentage,
        ROUND(
            SUM(CASE WHEN us.years_experience >= jr.min_years_required THEN jr.importance_level ELSE 0 END) * 100.0 / 
            SUM(jr.importance_level), 2
        ) AS weighted_match_percentage
    FROM recommendation_events re
    LEFT JOIN job_requirements jr ON re.job_id = jr.job_id
    LEFT JOIN user_skills us ON re.user_id = us.user_id AND jr.required_skill = us.skill_name
    GROUP BY re.recommendation_id, re.user_id, re.job_id
),

match_score_buckets AS (
    -- æŒ‰åŒ¹é…åº¦åˆ†æ¡¶åˆ†æ
    SELECT 
        CASE 
            WHEN sm.weighted_match_percentage >= 80 THEN '80-100%'
            WHEN sm.weighted_match_percentage >= 60 THEN '60-79%'
            WHEN sm.weighted_match_percentage >= 40 THEN '40-59%'
            WHEN sm.weighted_match_percentage >= 20 THEN '20-39%'
            ELSE '0-19%'
        END AS match_score_bucket,
        COUNT(*) AS total_recommendations,
        SUM(re.clicked) AS total_clicks,
        SUM(re.applied) AS total_applications,
        ROUND(AVG(re.relevance_rating), 2) AS avg_relevance_rating
    FROM recommendation_events re
    JOIN skill_matching sm ON re.recommendation_id = sm.recommendation_id
    WHERE re.relevance_rating IS NOT NULL
    GROUP BY 
        CASE 
            WHEN sm.weighted_match_percentage >= 80 THEN '80-100%'
            WHEN sm.weighted_match_percentage >= 60 THEN '60-79%'
            WHEN sm.weighted_match_percentage >= 40 THEN '40-59%'
            WHEN sm.weighted_match_percentage >= 20 THEN '20-39%'
            ELSE '0-19%'
        END
)

-- ä¸»æŸ¥è¯¢ï¼šæ¨èç²¾å‡†åº¦åˆ†æ
SELECT 
    match_score_bucket,
    total_recommendations,
    total_clicks,
    total_applications,
    -- è®¡ç®—ç‡
    ROUND(total_clicks * 100.0 / total_recommendations, 2) AS click_rate,
    ROUND(total_applications * 100.0 / total_recommendations, 2) AS application_rate,
    avg_relevance_rating
FROM match_score_buckets
ORDER BY 
    CASE match_score_bucket
        WHEN '80-100%' THEN 1
        WHEN '60-79%' THEN 2  
        WHEN '40-59%' THEN 3
        WHEN '20-39%' THEN 4
        WHEN '0-19%' THEN 5
    END;`,
    tags: 'æ¨èç²¾å‡†åº¦,æŠ€èƒ½åŒ¹é…,èŒä½åˆ†æ',
    source: 'LeetCode Discuss',
    year: 2024,
    isVerified: true
  }
];

async function addPythonSqlQuestions() {
  console.log('ğŸ’» å¼€å§‹æ·»åŠ Python/SQLç¼–ç¨‹é¢˜ç›®...\n');
  
  try {
    // 1. å…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰technicalç±»å‹çš„é¢˜ç›®
    const existingTechnical = await sql`
      SELECT COUNT(*) as count FROM interview_questions WHERE question_type = 'technical'
    `;
    
    console.log(`ğŸ“Š å½“å‰technicalé¢˜ç›®æ•°é‡: ${existingTechnical[0].count}`);
    
    // 2. å¦‚æœæ²¡æœ‰technicalé¢˜ç›®ï¼Œå…ˆå°†ä¸€äº›case_studyæ”¹å›technicalï¼ˆè¿™äº›åº”è¯¥æ˜¯ç¼–ç¨‹é¢˜ï¼‰
    if (existingTechnical[0].count === 0) {
      console.log('ğŸ”„ å°†ç°æœ‰çš„case_studyä¸­çš„ç¼–ç¨‹é¢˜é‡æ–°åˆ†ç±»ä¸ºtechnical...');
      await sql`
        UPDATE interview_questions 
        SET question_type = 'case_study' 
        WHERE question_type = 'technical'
      `;
    }
    
    // 3. æ’å…¥æ–°çš„Python/SQLç¼–ç¨‹é¢˜ç›®
    console.log('ğŸ“ æ’å…¥æ–°çš„Python/SQLç¼–ç¨‹é¢˜ç›®...');
    
    for (let i = 0; i < pythonSqlQuestions.length; i++) {
      const q = pythonSqlQuestions[i];
      
      try {
        await sql`
          INSERT INTO interview_questions (
            company, position, question_type, difficulty, question, 
            recommended_answer, tags, source, year, is_verified
          ) VALUES (
            ${q.company}, ${q.position}, ${q.questionType}, ${q.difficulty}, 
            ${q.question}, ${q.recommendedAnswer}, ${q.tags}, ${q.source}, 
            ${q.year}, ${q.isVerified}
          )
        `;
        
        console.log(`âœ… æ’å…¥ç¼–ç¨‹é¢˜ç›® ${i + 1}: ${q.company} - ${q.position}`);
        console.log(`   ${q.question.substring(0, 60)}...`);
      } catch (error) {
        console.log(`âš ï¸  é¢˜ç›® ${i + 1} å¯èƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡: ${q.company} - ${q.position}`);
      }
    }
    
    // 4. ç»Ÿè®¡æœ€ç»ˆç»“æœ
    console.log('\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ç»“æœ:');
    
    const typeStats = await sql`
      SELECT question_type, COUNT(*) as count 
      FROM interview_questions 
      GROUP BY question_type 
      ORDER BY count DESC
    `;
    
    console.log('\nğŸ“ é¢˜ç›®ç±»å‹åˆ†å¸ƒ:');
    typeStats.forEach(stat => {
      console.log(`   ${stat.question_type}: ${stat.count} é“é¢˜ç›®`);
    });
    
    const technicalStats = await sql`
      SELECT company, position, COUNT(*) as count, LEFT(question, 50) as sample_question
      FROM interview_questions 
      WHERE question_type = 'technical'
      GROUP BY company, position, question
      ORDER BY company, position
    `;
    
    console.log('\nğŸ’» technicalé¢˜ç›®è¯¦æƒ…:');
    technicalStats.forEach(stat => {
      console.log(`   ${stat.company} - ${stat.position}: ${stat.sample_question}...`);
    });
    
    const totalCount = await sql`SELECT COUNT(*) as total FROM interview_questions`;
    console.log(`\nğŸ‰ æ•°æ®åº“ä¸­ç°æœ‰æ€»é¢˜ç›®æ•°: ${totalCount[0].total} é“\n`);
    
  } catch (error) {
    console.error('âŒ æ·»åŠ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:', error);
  }
}

export { addPythonSqlQuestions };

// å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if (require.main === module) {
  addPythonSqlQuestions();
} 